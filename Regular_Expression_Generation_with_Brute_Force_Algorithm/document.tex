\documentclass[10pt,twocolumn]{article}

% use the oxycomps style file
\usepackage{oxycomps}

% usage: \fixme[comments describing issue]{text to be fixed}
% define \fixme as not doing anything special
\newcommand{\fixme}[2][]{#2}
% overwrite it so it shows up as red
\renewcommand{\fixme}[2][]{\textcolor{red}{#2}}
% overwrite it again so related text shows as footnotes
%\renewcommand{\fixme}[2][]{\textcolor{red}{#2\footnote{#1}}}

% read references.bib for the bibtex data
\bibliography{references}

% include metadata in the generated pdf file
\pdfinfo{
    /Title (hi)
    /Author (John Kim)
}

% set the title and author information
\title{Regular Expression Generation with Brute-Force Algorithm}
\author{John Kim}
\affiliation{Occidental College}
\email{jkim4@oxy.edu}

\begin{document}

\maketitle

\section{Abstract}
Regular Expression, also known as Regex, is a sequence of symbols, numbers, and characters that represents a search pattern and/or text manipulations. It is highly versatile and widely supported across many programming languages. 

This paper aims to automate the task of generating regex utilizing a brute force algorithm. Given an example of an regex output the code will conduct pre-processing of relevant keywords to simplify the task and find the search-pattern utilizing a brute force algorithm.

\section{Technical Background}
\subsection{Byte Pair Encoding}
In natural language processing, tokenization is the process of converting text to smaller interchangeable units called tokens. These tokens can then be re-arranged to form completely new sentences. 

Byte Pair Encoding is a well established tokenization algorithm. It finds and merges the most common pairs of adjacent characters in a given corpus until the word reaches a set size, creating  tokens that efficiently represent text\cite{zouhar-etal-2023-formal}. BPE is an algorithm designed with Natural Language Processing in mind and did not perform seamlessly as you could expect from a text based corpus. BPE algorithm was utlized in a corpus of regular expressions\cite{huggingface}. Some manual filtering was required afterwards, but it was able to recognize common token pairs such as \texttt{.*} 

\subsection{Standard Deviation}
Standard deviation is a statistical measurement that quantifies the amount of variation in a set of values. A low standard deviation indicates that the data points tend to be close to the mean, whereas a high standard deviation indicates that the values are more spread out over a wider range. Standard Deviation was utilized in conjunction with the brute force algorithm. Its purpose is to randomly determine the number of token used in the algorithm with a statistical focus on the mean. The algorithm assumes a mean of 3 tokens and gradually increases the value of standard deviation to increase. Eventually, as the graph nears a flat line, a random number between 1 and 10 is chosen as the token count.

\subsection{Regex-based Denial of Service (ReDoS)}
ReDoS is a DoS attack caused by algorithmic complexity that drains a given web-services’ resources due to it costly time complexity. The time complexity can grow exponentially or polynomially as its input size increases linearly. 

Most regex functions have a exponential time worst-case complexity but they are often overlooked during development as they are often, but not always, rarely triggered with genuine inputs. ReDoS failures occur when such regex function is given a maliciously crafted input or a genuine input that triggers exponential/polynomial backtracking. Programming languages predominantly use a backtracking for its regex search algorithm, most notable examples are C\#, Java, JavaScript, and Python. Its computational cost is negligible in most use cases, but it can sometimes be detrimental.

There are numerous ways to write a regular expression which gives identical outputs. A regex generated by the brute force algorithm may work but may also be computationally inefficient to the point of overloading the cpu.

\subsubsection{2019 Cloudflare Outage}

Sometimes a regular expression function can consistently cause a ReDoS failures even on genuine inputs. One such example is the 2019 Cloudflare global outage. Cloudflare is a company that provides a number of internet services one of which is Content Delivery Network(CDN) services. To combat Cross-site Scripting attacks through their CDN, they implemented a regular expression to their fire-wall that detects any Java-Script and HTML code appended to URLs. 

\begin{verbatim}
(?:(?:\"|'|\]|\}|\\|\d|(?:nan|infinity|
true|false|null|undefined|symbol|math)|
\`|\-|\+)+[)]*;?((?:\s|-|~|!|{}|\|\||\+
)*.*(?:.*=.*)))
\end{verbatim}


This was the regular expression in question. The malformed section that caused the outage was:
\begin{verbatim}
.*(?:.*=.*)
\end{verbatim}
Now disregarding the non-capturing group: 
\begin{verbatim}
(?:)
\end{verbatim}
It can be disregarded since the regex still needs the computational resources to conduct the search and then ignore it. We are left with:
\begin{verbatim}
.*.*=.*
\end{verbatim}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{matching-x-x.png}
    \caption{exponential time complexity of Cloudflare's malformed regex}
    \label{fig:placeholder}
    \cite{cloudflare2019}
\end{figure}
As the simplified regex shows, the first two .* will match greedily and then both would backtrack iterating backwards one unit at a time until it finds an equal-sign or until all possibilities are exhausted. This means that it will take 23 steps to match an input string simple as ‘x=x’. 33 steps for ‘x=xx’ and 45 for ‘x=xxx’\cite{cloudflare2019} The complexity is exponential and much worse if the string does not contain an equal-sign as it would need to exhaust all possible combination of the two .* For instance, it will take 4,067 steps to compute a string that is 20 characters long without an equal-sign\cite{cloudflare2019}. Considering how long URLs can get, you can see how this would cause an outage. 

The outage demonstrates the disastrous potential of ReDoS failures and how inconspicuous they can be to users. Unfortunately, the program is not able to detect and mitigate ReDoS failures.

\subsubsection{Linear Time Regular Expression}

Since the 2019 outage, Cloudflare has switched to the Rust regex engine for their regex needs. The Rust engine is deterministic and runs in linear time: $O(N)$. 

Deterministic regular expressions such as re2 and Rust regex engine achieves linear time complexity by utilizing a combination of Deterministic Finite Automata(DFA) and Non-Deterministic Finite Automata(NFA), allowing it to avoid backtracking\cite{deterministic}. The primary limitation of such engines is the high memory requirements for complex regexs.

Users are encouraged to use a linear time engine. It is possible for the brute force algorithm generating regex with exponential time worst-case complexity which may lead to a ReDoS failure.

\subsection{Prior Work}
Prior works in the field of automated Regular Expression generation is limited. This may speak to its difficulty or insignificance even if successfully implemented. I suspect both.

\subsubsection{Genetic Programming for Regex Generation}
A team of researchers at the University of Trieste in Italy utilized genetic programming to automate regular expression generation as a so called proof of concept. Their findings were limited. Their research did demonstrate an abstract potential for such algorithm. However, it only demonstrated that it’s able to utilize genetic programming to generate regular expressions with manual post-processing. Their proof of concept failed to independently generate any coherent regular expression. Furthermore, their algorithm was tested on a very limited sample size of two.\cite{geneticprogramming}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{Picture1.png}
    \caption{Keyword Pre-Processing Examples}
    \label{fig:placeholder}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Picture9.png}
    \caption{program overview}
    \label{fig:placeholder}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{UI pic.png}
    \caption{User Interface}
    \label{fig:placeholder}
\end{figure}

\subsection{Methods}
The program conducts pre-processing of keywords provided based on the user inputs in order to simplify it as much as possible. Then the brute force algorithm, utilizing both Byte Pair Encoded tokens and Standard Deviation number generation, generates potential regular expressions. Once the correct regex is found, it is then outputted. 

The shorter token list is used for the Brute Force Algorithm which contains the most common regular expression tokens and when the list fails to generate the correct regex the complete list is used.

\begin{table}
    \footnotesize
    \centering
    \begin{tabular}{r|cl}
         \textbf{ }  &  \textbf{ average time(seconds)}    & \textbf{ average attemtps}            \\
        \hline
        \textbf{\texttt{\textbackslash d[Gg]upp(ies|y)?\$}}  &  1.37 & 126,026.2\\
        \textbf{\texttt{\^}\texttt{d[Gg]upp(ies|y)?\$}}  &  28.33 & 2,610,317.7 \\
        \textbf{\texttt{\textbackslash b\textbackslash D[Gg]upp(ies|y)?\textbackslash b}}  &  37.57 & 3,389,479.3 \\
        \textbf{\texttt{\^}\texttt{(John|Han|Luke|Leia)\$}}  &  0.05 & 4,435.4 \\
    \end{tabular}
    \caption{Average of 10 Trials}
    \label{tbl:timeline}
\end{table}
\subsection{results}[]
The results were averaged from 10 trials. The results show that brute force algorithm is able to consistently generate the correct regular expression when the non-keyword tokens are limited to 5 maximum. The algorithm, however, was not able to reliably generate longer more complex regular expressions.





\printbibliography

\appendix

\clearpage

\onecolumn
\end{document}
